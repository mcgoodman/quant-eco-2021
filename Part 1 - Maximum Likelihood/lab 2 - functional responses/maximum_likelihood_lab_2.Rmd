---
title: 'Maximum likelihood Lab 2: Functional Responses'
author: "< your name here >"
date: "BIOHOPK 143H - Winter 2021"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.height = 4, fig.width = 6, message = FALSE, warning = FALSE)

### load the tidyverse packages
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)

### load the "emdbook" package containing data from Bolker 2017
if (!require("emdbook")) install.packages("emdbook"); library(emdbook)
```


Aiming to characterize the trade-off between growth and survival, Vonesh and Bolker (2005) use experimental manipulations to estimate the effects of prey (tadpole) size and prey density on the rate of consumption by aquatic predators (a species of dragonfly, the "keyhole glider"). Here, we'll model the relationship between tadpole density and predation by fitting two forms of the functional response using their experimental data.

Recall that a **functional response** defines the relationship between prey density or number (the independent variable) and the rate of prey consumption by a predator (the dependent variable) - so, our x variable is the number of tadpole larvae in each tank, and our y variable is the number of tadpoles eaten over the study period. We have a fixed number of predators in each tank (3 dragonflies), and a fixed amount of time (14 days) that we observe our tadpoles for.

We might be tempted to just take the Normal negative log-likelihood functions we made for fitting regression models yesterday, and replace the regression equation with one of the many available functional response equations - but we can do better than this.

Specifically - we should use a binomial likelihood function (instead of a Normal one), for a few reasons. Take a minute to look up the binomial distribution and list a couple reasons why it's a better choice for this data than a Normal distribution:

> 

What are the parameters of the binomial distribution? List and describe them here:

> 


The data we need for this part of the lab is included in the `emdbook` package as `ReedfrogFuncresp` - the `Initial` column indicates the initial number of prey in the tank, and the `Killed` column indicates the number of prey consumed by the predator.

First, let's plot the relationship between the number of tadpoles eaten and the initial number of tadpoles:

```{r}
data(ReedfrogFuncresp)

ReedfrogFuncresp %>% 
  ggplot(aes(Initial, Killed)) + 
  geom_point()
```

# The Holling Type II response

There are many hypothesized forms of the functional response, but perhaps the most ubiquitous is the Holling Type II response, which models the decline in predation rate at high prey densities as arising from two parameters: the **attack rate** of the predator, and the **handling time**, which is how long it takes a predator to consume an individual prey item. At low prey densities, the attack rate controls how many prey a predator consumes, but as prey density increases, the handling time becomes limiting, and the predation rate approaches as asymptote:

$$
N_c =  P \times t \times \frac{a N_0}{1 + a h N_0}
$$

Where $N_c$ is the number of prey consumed, $N_0$ is the initial number of prey, $a$ is the attack rate, $h$ is the handling time. $P$ is the number of predators, and $t$ is the amount of time over which predation is observed - these variables just scale the functional response.

It's straightforward to express the Holling Type II response as an R function:

```{r}
## Function describing Holling Type II functional response
holling2 <- function(N0, a, h, P, t) {
  P * t * (a * N0)/(1 + a * h * N0)
}
```

Now, let's come up with an equation for the likelihood of the data given the likelihood function of choice and our functional response:

$$
\begin{align*}
  N_c &\sim \text{Binomial}(N_0, p_c) \\
  p_c &=  \frac{P \times t \times \frac{a N_0}{1 + a h N_0}}{N_0}
\end{align*}
$$

Here's a function for the negative log-likelihood under this model - take a minute to understand the code. The attack rate $a$ and the handling time $h$ can't be negative, so we'll just return a really high value for those that are. Also, we know the number of predators $P$ in each tank (3 dragonflies) and the amount of time $t$ for which predation is observed (14 days), so we'll pass these as separate arguments instead of including them in our `par` argument:

```{r}
## Negative log likelihood
nll_holling2 <- function(par, P, t, N0, Nc) {
  
  a <- par[1] ## attack rate
  h <- par[2] ## handling time
  
  if (a >= 0 & h >= 0) {
    
    p_sim = holling2(N0, a, h, P, t)/N0
    return(-sum(dbinom(Nc, prob = p_sim, size = N0, log = TRUE)))
  } else {
    return(1e5) # return a large number for nonsensical parameter values
  }
  
}
```

# Choosing initial values

When we ran a linear regression on Monday, we didn't pay much attention to the initial parameter values that we passed to `optim`. However, for more complex functions, they can matter quite a bit - if the initial values are too off the mark, the optimization algorithm may never find the maximum likelihood estimates. So, let's try to make a visual guess as to what the initial values should be.

Taking a look at the Hollling Type II equation, we can see that for small values of $N_0$ (the initial prey density), the number of prey eaten *per predator, and per unit time* (i.e. ignoring the $P$ and $t$ parameters) is close to $a$ (the attack rate) and as $N_0$ increases, the number of prey eaten by each predator reaches an asymptote of $1/h$.

Let's plot the number of prey eaten, divided by the numer of predators $P$ and the study time $t$, as a function of the intial prey density: 

```{r}
ReedfrogFuncresp$p_killed <- ReedfrogFuncresp$Killed / (3 * 14)

ReedfrogFuncresp %>% 
  ggplot(aes(Initial, p_killed)) + 
  geom_point()
```

The asymptote looks to be around 0.85, and we can guess $a$ by fitting a linear regression on smaller values of the initial population size, holding the intercept at 0 (since predators can't consume prey that aren't there):

```{r}
initial_lm <- lm(
  p_killed ~ 0 + Initial, 
  data = ReedfrogFuncresp %>% filter(Initial < 20)
)

coef(initial_lm)
```

# Maximum likelihood estimates

So, our rough initial values are $a = 0.014$ and $h = 0.85$. Let's pass these to `optim`, along with our `nll_holling2` function and our data, to obtain maximum-likelihood estimates for the attack rate and handling time: 
```{r}
holling_mle <- optim(
  par = c(a = 0.014, h = 0.85), 
  fn = nll_holling2, 
  N0 = ReedfrogFuncresp$Initial, 
  Nc = ReedfrogFuncresp$Killed, 
  P = 3, 
  t = 14, 
  hessian = TRUE
)

holling_mle
```

Let's superimpose the fitted curve on top of our data:

```{r}
## obtain fitted values
fitted <- data.frame(initial = seq(0, 100, 0.1)) ## initial prey density
fitted$holling2 <- holling2(fitted$initial, holling_mle$par[1], holling_mle$par[2], P = 3, t = 14)

ReedfrogFuncresp %>% 
  ggplot(aes(Initial, Killed)) + 
  geom_point() + 
  geom_line(aes(x = initial, y = holling2), data = fitted) + 
  labs(color = "type")
```


We can obtain standard errors for the attack rate and handling time with Fisher information:

```{r}
var_cov <- solve(holling_mle$hessian)
se <- sqrt(diag(var_cov))
se
```

# Likelihood Surface

We only have two parameters in this model, so let's visualize the dependency of our likelihood function on the parameters as a **surface**. Fill in the for loop below so that each combination of parameters has a corresponding negative log-likelihood value:

```{r }
a_seq <- seq(0.002, 0.02, length.out = 100)
h_seq <- seq(0.05, 2, length.out = 100)

par_grid <- expand.grid(log_a = log(a_seq), log_h = log(h_seq))

for (i in 1:nrow(par_grid)) {
  par_grid$nll[i] <- ## YOUR CODE HERE
}

par_grid %>% 
  ggplot(aes(log_a, log_h, z = nll)) + 
  geom_contour_filled() + 
  geom_point(aes(x = holling_mle$par[1], y = holling_mle$par[2]), color = "white")
```

Based on the shape of the likelihood surface, do you think a normal approximation for the confidence intervals on $a$ and $h$ would be appropriate? Why or why not?

> 

# Confidence intervals

For this example, let's use the bootstrap to obtain confidence intervals for $a$ and $h$, and to obtain confidence intervals for the response variable (the number of prey consumed). Since there aren't a ton of observations (16) and the observations are experimental manipulations (not random samples), the bootstrap isn't necessarily the *best* choice, but we'll use it anyway (probably, likelihood profiles would be the best way to compute confidence intervals here). 

We'll take 10,000 samples of the attack rate and handling time:

```{r}
n_obs <- nrow(ReedfrogFuncresp) ## number of observations
n_boot <- 10000 ## number of bootstrap samples

## matrix to store attack rate and handling time
holling_boot <- matrix(NA, nrow = n_boot, ncol = 2)

for (i in 1:n_boot) {
  sample_rows <- sample(1:n_obs, n_obs, replace = TRUE) ## rows to sample
  sample_data <- ReedfrogFuncresp[sample_rows,]
  
  sample_mle <- optim(
    par = holling_mle$par, ## initialize at the overall MLE
    fn = nll_holling2, 
    N0 = sample_data$Initial, 
    Nc = sample_data$Killed, 
    P = 3, 
    t = 14
  )
  
  holling_boot[i,] <- sample_mle$par
}
```

Plot histograms of these samples. The attack rate samples are in the first column, and the handling time samples are in the second. What do you notice about the distributions of each of these parameters?

> 

```{r}
## YOUR CODE HERE
```

Obtain 95% confidence intervals using (1) the bootstrap distribution, and (2) the MLE and Fisher standard errors. How do these compare?

> 

```{r}
## YOUR CODE HERE - bootstrap confidence intervals
```

```{r}
## YOUR CODE HERE - normal approximation with fisher SE
```

Now, let's use our samples of $a$ and $h$ to draw confidence bands for the number of prey eaten. To do that, we'll loop over the rows of the matrix containing our sample estimates, and for each we'll store a fitted line by applying the `holling2` function we defined earlier on to initial prey densities up to 100:

```{r}
sample_fits <- vector("list", n_boot)

for (i in 1:n_boot) {
  sample_fits[[i]] <- data.frame(
    initial = 0:100, 
    killed = holling2(0:100, a = holling_boot[i,1], h = holling_boot[i,2], P = 3, t = 14)
  )
}

sample_fits <- do.call("rbind", sample_fits)
```

Now, let's use the `quantile` function on each of the different initial prey densities to obtain 80% and 95% confidence bands and plot them:

```{r}
holling_confint <- sample_fits %>% 
  group_by(initial) %>% 
  summarize(`2.5%` = quantile(killed, 0.025),
            `10%` = quantile(killed, 0.1),
            `90%` = quantile(killed, 0.9), 
            `97.5%` = quantile(killed, 0.975))

ReedfrogFuncresp %>% 
  ggplot(aes(Initial, Killed)) + 
  geom_point() + ## points for observed data
  geom_ribbon(aes(x = initial, ymin = `2.5%`, ymax = `97.5%`), ## 95% confidence band
              alpha = 0.2, fill = "dodgerblue3", inherit.aes = FALSE,
              data = holling_confint) + 
  geom_ribbon(aes(x = initial, ymin = `10%`, ymax = `90%`), ## 80% confidence band
              alpha = 0.4, fill = "dodgerblue3", inherit.aes = FALSE,
              data = holling_confint) + 
  geom_line(aes(x = initial, y = holling2), color = "dodgerblue3", ## MLE
            size = 1, data = fitted) + 
  labs(x = "initial prey density", y = "number of prey killed") + 
  theme_bw()
```

# Exercises

## 1. Likelihood ratio tests

Use a likelihood ratio test to assess the null hypothesis that the handling time $h$ is equal to 1 (i.e., that it takes a dragonfly about 1 day to eat/digest a tadpole). Refer back to the first assignment for how to do this - it involves obtaining a maximum likelihood estimate for $a$ while holding $h$ constant.

```{r}
## YOUR CODE HERE
```

## 2. The Rogers functional response

An alternative model, which allows for depletion of the tadpoles over the time during which the study is conducted, is the **Rogers random-predator equation**: 

$$
N = N_0 \left( 1 - e^{a(Nh - PT)} \right)
$$
From Chapter 8 of Ben Bolker's book: 

The Rogers random-predator equation (8.1.4) contains N on both the left- and right-hand sides of the equation; traditionally, one has had to use iterative numerical methods to compute the function (Vonesh and Bolker,
2005). However, the Lambert W function (Corless et al., 1996), which gives the solution to the equation $W(x)e^{W(x)} = x$, can be used to compute the Rogers equation efficiently: in terms of the Lambert W the Rogers equation is:

$$
N = N_0 - \frac{W(ahN_0 e^{-a(PT - hN_0)})}{ah}
$$
The `lambertW` function from the `emdbook` package allows us to compute this version of the Roger's random predator equation. A negative log-likelihood function that uses the Rogers model is: 

```{r}
rogers <- function(N0, a, h, P, t) {
  N0 - lambertW(a * h * N0 * exp(-a*(P * t - h * N0)))/(a * h)
}

nll_rogers <- function(par, P, t, N0, Nc) {
  
  a <- par[1]; h <- par[2]
  
  if (a >= 0 & h >= 0 ) {
    p_sim = rogers(N0, a, h, P, t)/N0
    return(-sum(dbinom(Nc, prob = p_sim, size = N0, log = TRUE)))
  } else {
    return(1e5)
  }
  
}
```

Obtain maximum-likelihood estimates for $a$ and $h$ under the Rogers model (Use the maximum-likelihood estimates for $a$ and $h$ from the Holling fit as starting parameters). Are the parameter estimates different? Why do you think they are / are not?

> 

```{r}
## YOUR CODE HERE
```

We can use the Akaike Information Criterion (AIC) to compare these models. We prefer the model with the lowest AIC, but only if that model has an AIC value at least two units smaller than the other models (i.e., if two models are within 2 AIC of each other, neither one is considered to fit the data substantially better). The equation for AIC is: 

$$
AIC = 2k - 2\ln \mathcal{L}(\hat{\theta})
$$
where $k$ is the number of parameters in the model (2, for both models), and $- 2\ln \mathcal{L}(\hat{\theta})$ is twice the value of the negative log likelihood at the MLE. Compute the AIC for both models (the negative log likelihood at the MLE is returned from `optim` under the `$value` object).

```{r}
## YOUR CODE HERE
```

Does AIC prefer either model? Based on AIC and what you know of the study, which model would you prefer?

> 



